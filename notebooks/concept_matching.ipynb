{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99464a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d38f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SDXL text encoder\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe_sd_turbo = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "text_encoder = pipe_sd_turbo.encode_prompt\n",
    "prompt = \"a photo of a cat\"\n",
    "\n",
    "prompt_embeds, _, pooled_prompt_embeds, _ = text_encoder(prompt, device=device)\n",
    "prompt_embeds.shape, pooled_prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://proceedings.neurips.cc/paper_files/paper/2024/file/996bef37d8a638f37bdfcac2789e835d-Paper-Conference.pdf\n",
    "# https://github.com/AI4LIFE-GROUP/SpLiCE\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "# load vocabulary\n",
    "SUPPORTED_VOCAB = [\n",
    "    \"laion\",\n",
    "    \"laion_bigrams\",\n",
    "    \"mscoco\"\n",
    "]\n",
    "\n",
    "GITHUB_HOST_LINK = \"https://raw.githubusercontent.com/AI4LIFE-GROUP/SpLiCE/main/data/\"\n",
    "\n",
    "\n",
    "def _download(url: str, root: str, subfolder: str):\n",
    "    root_subfolder = os.path.join(root, subfolder)\n",
    "    os.makedirs(root_subfolder, exist_ok=True)\n",
    "    filename = os.path.basename(url)\n",
    "    download_target = os.path.join(root_subfolder, filename)\n",
    "\n",
    "    if os.path.isfile(download_target):\n",
    "        return download_target\n",
    "\n",
    "    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n",
    "        while True:\n",
    "            buffer = source.read(8192)\n",
    "            if not buffer:\n",
    "                break\n",
    "            output.write(buffer)\n",
    "    return download_target\n",
    "\n",
    "\n",
    "def get_vocabulary(name: str, vocabulary_size: int, download_root = None):\n",
    "    if name in SUPPORTED_VOCAB:\n",
    "        vocab_path = _download(os.path.join(GITHUB_HOST_LINK, \"vocab\", name + \".txt\"), download_root or os.path.expanduser(\"~/.cache/splice/\"), \"vocab\")\n",
    "\n",
    "        vocab = []\n",
    "        with open(vocab_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if vocabulary_size > 0:\n",
    "                lines = lines[-vocabulary_size:]\n",
    "            for line in lines:\n",
    "                vocab.append(line.strip())\n",
    "        return vocab\n",
    "    else:\n",
    "        raise RuntimeError(f\"Vocabulary {name} not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_name = \"mscoco\"\n",
    "vocab_size = -1\n",
    "vocab = get_vocabulary(vocab_name, vocab_size)\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text embeddings for concepts in the vocabulary\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_concept_embeddings(text_encoder, vocab: list[str], device = \"cuda\"):\n",
    "\tconcepts = []\n",
    "\n",
    "\tfor concept in tqdm(vocab, desc=\"Getting concept embeddings\", total=len(vocab)):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tprompt_embeds, _, pooled_prompt_embeds, _ = text_encoder(concept, device=device)\n",
    "\t\t\tconcept_embedding = pooled_prompt_embeds\n",
    "\t\tconcepts.append(concept_embedding)\n",
    "\t\n",
    "\tconcepts = torch.stack(concepts).squeeze()\n",
    "\t# concepts = F.normalize(torch.stack(concepts).squeeze(), dim=1)\n",
    "\t# concepts = F.normalize(concepts-torch.mean(concepts, dim=0), dim=1)\t\n",
    "\treturn concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34084687",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings = get_concept_embeddings(text_encoder, vocab, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4856417",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(concept_embeddings, f\"{vocab_name}_{vocab_size if vocab_size > 0 else 'all'}_concept_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f97dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SAE\n",
    "from src.sae.sae import Sae\n",
    "\n",
    "ckpt_path = (\n",
    "    \"checkpoints/coco2017/sdxl-turbo/batch_topk_expansion_factor16_k32_multi_topkFalse_auxk_alpha0.03125_output_249_output\"\n",
    ")\n",
    "hookpoint = \"down_blocks.2\"\n",
    "\n",
    "sae = Sae.load_from_disk(\n",
    "\tos.path.join(\n",
    "\t\tckpt_path,\n",
    "\t\thookpoint,\n",
    "\t),\n",
    "\tdevice=device,\n",
    ").to(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cached SAE activations\n",
    "from datasets import Dataset\n",
    "\n",
    "num_timesteps = 4\n",
    "activations_dataset_path = f\"activations/coco2017/sdxl-turbo/steps{num_timesteps}\"\n",
    "\n",
    "activations_dataset = Dataset.load_from_disk(\n",
    "\tos.path.join(activations_dataset_path, hookpoint), keep_in_memory=False\n",
    ")\n",
    "activations_dataset.set_format(\n",
    "\ttype=\"torch\", columns=[\"activations\", \"timestep\", \"file_name\"], dtype=dtype\n",
    ")\n",
    "\n",
    "# filter dataset to only include activations from timestep 249\n",
    "activations_dataset = activations_dataset.filter(\n",
    "\tlambda x: x[\"timestep\"] == 249, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cfbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "# compute average activations per sample\n",
    "\n",
    "avg_activations_per_sample = torch.zeros(\n",
    "\t(len(activations_dataset), sae.num_latents), dtype=torch.float16\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "dl = torch.utils.data.DataLoader(\n",
    "\tactivations_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    ")\n",
    "with torch.no_grad():\n",
    "\tfor i, batch in tqdm(enumerate(dl), total=len(dl)):\n",
    "\t\tacts = batch[\"activations\"].to(sae.device)\n",
    "\t\tacts = einops.rearrange(\n",
    "\t\t\tacts,\n",
    "\t\t\t\"batch sample_size d_model -> (batch sample_size) d_model\",\n",
    "\t\t)\n",
    "\t\tout = sae.pre_acts(acts)\n",
    "\t\t# Reshape to get per-sample activations and compute mean for each sample\n",
    "\t\tout = out.view(\n",
    "\t\t\tbatch[\"activations\"].shape[0], -1, sae.num_latents\n",
    "\t\t)  # [batch, sample_size, num_latents]\n",
    "\t\tbatch_avg_activations = out.mean(dim=1).to(\n",
    "\t\t\tdtype=torch.float16\n",
    "\t\t)  # [batch, num_latents]\n",
    "\n",
    "\t\t# Store in the correct indices\n",
    "\t\tstart_idx = i * batch_size\n",
    "\t\tend_idx = min(start_idx + batch_size, len(activations_dataset))\n",
    "\t\tavg_activations_per_sample[start_idx:end_idx] = batch_avg_activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topk_activating_examples(activations_per_sample, latent_idx, k=10):\n",
    "\ttopk_indices = torch.argsort(\n",
    "\t\tactivations_per_sample[:, latent_idx], dim=0, descending=True\n",
    "\t)[:k]\n",
    "\treturn topk_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a latent neuron index\n",
    "# retrive the most activating samples for the neuron\n",
    "k = 10\n",
    "# latent_idx = 374 # ski\n",
    "# latent_idx = 6475 # kites\n",
    "# latent_idx = 6531 # faces\n",
    "# latent_idx = 73 # around the motorcycle\n",
    "# latent_idx = 97 # keyboard\n",
    "latent_idx = 123 # hands\n",
    "\n",
    "topk_indices = find_topk_activating_examples(\n",
    "\tavg_activations_per_sample, latent_idx, k\n",
    ")  # find topk samples containing patches with higest activations\n",
    "topk_samples = activations_dataset[topk_indices.tolist()]\n",
    "file_names_topk = topk_samples[\"file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "coco_dataset = load_dataset(\"phiyodr/coco2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter coco dataset to only include the topk samples\n",
    "coco_topk_samples = coco_dataset[\"validation\"].filter(\n",
    "\tlambda x: x[\"file_name\"] in file_names_topk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_samples_captions = [\" \".join(captions) for captions in coco_topk_samples['captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds, _, pooled_prompt_embeds, _ = text_encoder(topk_samples_captions, device=device)\n",
    "prompt_embeds.shape, pooled_prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_samples_caption_embeddings = pooled_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a PCA on the text embeddings and extract the first PC direction\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def svd_flip(u, v):\n",
    "\t# columns of u, rows of v\n",
    "\tmax_abs_cols = torch.argmax(torch.abs(u), 1)\n",
    "\n",
    "\ti = torch.arange(u.shape[2]).to(u.device)\n",
    "\t\n",
    "\tmax_abs_cols = max_abs_cols.unsqueeze(-1)  # just to match the dimensions for gather, but not necessary to expand further\n",
    "\tsigns = torch.sign(torch.gather(u, 1, max_abs_cols))\n",
    "\t# signs = torch.sign(u[ max_abs_cols, i])\n",
    "\tu *= signs\n",
    "\tv *= signs.view(v.shape[0], -1, 1)\n",
    "\treturn u, v\n",
    "\n",
    "class PCA(nn.Module):\n",
    "\t\"\"\"From https://github.com/shengliu66/VTI\"\"\"\n",
    "\tdef __init__(self, n_components):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_components = n_components\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef fit(self, X):\n",
    "\t\tif X.ndim == 2:\n",
    "\t\t\tn, d = X.size()\n",
    "\t\t\tX = X.unsqueeze(0)\n",
    "\t\telif X.ndim == 3:\n",
    "\t\t\t_, n, d = X.size()\n",
    "\t\tif self.n_components is not None:\n",
    "\t\t\td = min(self.n_components, d)\n",
    "\t\tself.register_buffer(\"mean_\", X.mean(1, keepdim=True))\n",
    "\t\tZ = X - self.mean_ # center\n",
    "\t\tU, S, Vh = torch.linalg.svd(Z, full_matrices=False)\n",
    "\t\tVt = Vh\n",
    "\t\tU, Vt = svd_flip(U, Vt)\n",
    "\t\tself.register_buffer(\"components_\", Vt[:,:d])\n",
    "\t\treturn self\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\treturn self.transform(X)\n",
    "\n",
    "\tdef transform(self, X):\n",
    "\t\tassert hasattr(self, \"components_\"), \"PCA must be fit before use.\"\n",
    "\t\treturn torch.matmul(X - self.mean_, self.components_.transpose(-2, -1))\n",
    "\n",
    "\tdef fit_transform(self, X):\n",
    "\t\tself.fit(X)\n",
    "\t\treturn self.transform(X)\n",
    "\n",
    "\tdef inverse_transform(self, Y):\n",
    "\t\tassert hasattr(self, \"components_\"), \"PCA must be fit before use.\"\n",
    "\t\treturn torch.matmul(Y, self.components_) + self.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1).to(topk_samples_caption_embeddings.device).fit(topk_samples_caption_embeddings.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_captions_embedding = (pca.components_.sum(dim=1,keepdim=True) + pca.mean_).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "similarities = F.cosine_similarity(pca_captions_embedding.expand_as(concept_embeddings), concept_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_indices = torch.argsort(similarities, descending=True)[:10]\n",
    "most_similar_indices = most_similar_indices.cpu().numpy()\n",
    "most_similar_concepts = [vocab[i] for i in most_similar_indices]\n",
    "most_similar_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583d866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-diff-autointerpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
